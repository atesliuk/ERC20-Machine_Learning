{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERC20_Machine_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvz5pcyqs47p"
      },
      "source": [
        "# importing required libraries\n",
        "import sys\n",
        "import numpy as numpy\n",
        "from pandas import read_csv, DataFrame\n",
        "import matplotlib.pyplot as pyplot\n",
        "! pip install scikit-learn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# libraries for ARIMA model\n",
        "! pip install pmdarima\n",
        "import pmdarima as pmdarima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# libraries for LSTM model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIGyOBpoaTnG"
      },
      "source": [
        "# Importing and pre-processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMys94URZjhq"
      },
      "source": [
        "# array of lags that will be analysed\n",
        "lags_arr = [24, 48, 72]\n",
        "\n",
        "# array of token symbols that will be analysed\n",
        "token_symbols = ['UNI', 'LINK', 'AAVE', 'MKR', 'LEO', 'COMP', 'GRT', 'HT', 'CEL', \n",
        "                 'CHZ', 'TEL', 'YFI', 'HOT', 'ENJ', 'MANA', 'QNT', 'BAT', 'SNX', 'NEXO', \n",
        "                 'BNT', 'CRV', 'CHSB', 'KCS', 'ZRX', 'UMA', 'ANKR', 'VGX', '1INCH']\n",
        "number_of_tokens = len(token_symbols)\n",
        "\n",
        "# importing data from csv files (before executing this step, csv data files should be uploaded)\n",
        "raw_datasets = []\n",
        "for symbol in token_symbols:\n",
        "  raw_datasets.append(read_csv('{}.csv'.format(symbol)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c_LUicQ24Bq"
      },
      "source": [
        "# scaling data and splitting datasets to training and test datasets\n",
        "training_set_ratio = 0.80\n",
        "data_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Trades']\n",
        "scalers = []\n",
        "training_datasets, test_datasets = [], []\n",
        "\n",
        "for i in range(number_of_tokens):\n",
        "  # separating dates from values, converting data to float type\n",
        "  raw_dataset_without_dates = raw_datasets[i][data_columns].astype('float')\n",
        "\n",
        "  # scaling data\n",
        "  scaler = MinMaxScaler()\n",
        "  training_set_size = int(len(raw_datasets[i])*training_set_ratio)\n",
        "  raw_training_dataset = raw_dataset_without_dates[:training_set_size]\n",
        "  scaler.fit(raw_training_dataset)\n",
        "  scaled_dataset = DataFrame(scaler.transform(raw_dataset_without_dates), columns=data_columns)\n",
        "  scalers.append(scaler)\n",
        "\n",
        "  # splitting scaled dataset to training and test datasets\n",
        "  training_datasets.append(scaled_dataset[:training_set_size])\n",
        "  test_datasets.append(scaled_dataset[training_set_size:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDdK5xetUGs9"
      },
      "source": [
        "#ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2rJXn-HO1ln"
      },
      "source": [
        "# determining optimal ARIMA parameters for each dataset\n",
        "ARIMA_params = []\n",
        "for i in range(number_of_tokens):\n",
        "  model = pmdarima.auto_arima(training_datasets[i]['Close'], \n",
        "                        test='adf',             # using Augmented Dickey-Fuller (ADF) test to find optimal 'd'\n",
        "                        d=None,                 # stating that the model should determine 'd' parameter\n",
        "                        start_p=1, max_p=3,     # setting range for potential 'p' parameters\n",
        "                        start_q=1, max_q=3,     # setting range for potential 'q' parameters\n",
        "                        error_action='ignore',  # stating that erros should not be printed\n",
        "                        suppress_warnings=True) # stating that warnings should not be printed\n",
        "  \n",
        "  # recording calculated optimal ARIMA parameters\n",
        "  ARIMA_params.append(model.order)\n",
        "  \n",
        "  # printing out the parameters for visibility\n",
        "  print(token_symbols[i] + ' - ' + str(model.order))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtGMiYPoZztk"
      },
      "source": [
        "# stating that warnings should not be printed\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# predicting future prices with ARIMA\n",
        "ARIMA_predictions_for_all_lags = []\n",
        "\n",
        "# for each lags option (i.e. for 24 lags, 48 lags, and 72 lags)\n",
        "for lags in lags_arr:\n",
        "  print('{} lags:'.format(lags))\n",
        "  ARIMA_predictions = []  \n",
        "\n",
        "  # for each token\n",
        "  for i in range(number_of_tokens):\n",
        "    # Extracting  closing prices data\n",
        "    test_dataset_close_prices = test_datasets[i]['Close'].to_numpy().tolist()\n",
        "    training_dataset_close_prices = training_datasets[i]['Close'].to_numpy().tolist()\n",
        "\n",
        "    # creating batches for testing\n",
        "    training_X = []\n",
        "    for j in range(len(test_dataset_close_prices)):\n",
        "      curr_data = []\n",
        "      if j >= lags:\n",
        "        curr_data.extend(test_dataset_close_prices[(j-lags):j])\n",
        "      else:\n",
        "        curr_data.extend(training_dataset_close_prices[-(lags-j):])\n",
        "        curr_data.extend(test_dataset_close_prices[0:j])\n",
        "      training_X.append(curr_data)\n",
        "\n",
        "    # forecasting future prices from the test batches with ARIMA\n",
        "    curr_predictions = []\n",
        "    for j in range(len(test_dataset_close_prices)):\n",
        "      sys.stdout.write('\\r' + '{} - {}/{}'.format(token_symbols[i], j, len(test_dataset_close_prices)))\n",
        "      sys.stdout.flush()\n",
        "      model = ARIMA(training_X[j], order=ARIMA_params[i])\n",
        "      model_fit = model.fit()\n",
        "      output = model_fit.forecast()\n",
        "      yhat = output[0]\n",
        "      curr_predictions.append(yhat)\n",
        "    \n",
        "    # recording predictions for a token\n",
        "    ARIMA_predictions.append(curr_predictions)\n",
        "    print(' - done\\n')\n",
        "\n",
        "  # recording predictions for all tokens\n",
        "  ARIMA_predictions_for_all_lags.append(ARIMA_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ls7Ptc5r1UB"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL_uDiTDDw-q"
      },
      "source": [
        "# creating batches for training and testing\n",
        "training_y_arr, training_X_arr, test_X_arr = [], [], []\n",
        "\n",
        "for lags in lags_arr:\n",
        "  # arrays for storing batches for trianing and testing\n",
        "  training_X, test_X = [], []\n",
        "\n",
        "  # array for storing dependent variables of the training dataset\n",
        "  training_y = []\n",
        "\n",
        "  for i in range(number_of_tokens):\n",
        "    # extracting dependent variables of the training dataset\n",
        "    training_y.append(training_datasets[i][lags:]['Close'].to_numpy())\n",
        "\n",
        "    # creating batches for training\n",
        "    curr_training_X = []\n",
        "    for j in range(0, len(training_datasets[i]) - lags):\n",
        "      curr_training_X.append(training_datasets[i][j:j+lags].to_numpy().tolist())\n",
        "    training_X.append(numpy.array(curr_training_X))\n",
        "\n",
        "    # creating batches for testing\n",
        "    curr_test_X = []\n",
        "    for j in range(len(test_datasets[i])):\n",
        "      curr_data = []\n",
        "      if j >= lags:\n",
        "        curr_data.extend(test_datasets[i][(j-lags):j].to_numpy().tolist())\n",
        "      else:\n",
        "        curr_data.extend(training_datasets[i][-(lags-j):].to_numpy().tolist())\n",
        "        curr_data.extend(test_datasets[i][0:j].to_numpy().tolist())\n",
        "      curr_test_X.append(curr_data)\n",
        "    test_X.append(numpy.array(curr_test_X))\n",
        "\n",
        "  # recording dependent variables of the training datase\n",
        "  training_y_arr.append(training_y)\n",
        "\n",
        "  # recording training and testing batches\n",
        "  training_X_arr.append(training_X)\n",
        "  test_X_arr.append(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHPbxn0fqxp"
      },
      "source": [
        "# array for storing all LSTM predictions results\n",
        "LSTM_predictions_for_all_lags = []\n",
        "\n",
        "# for each lag option\n",
        "for l in range(len(lags_arr)):\n",
        "  LSTM_predictions = []\n",
        "  print('{} lags:'.format(lags_arr[l]))\n",
        "\n",
        "  # for each token building LSTM models and predicting test values\n",
        "  for i in range(number_of_tokens):\n",
        "    # building an LSTM model\n",
        "    training_data = training_X_arr[l][i]\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=(training_data.shape[1], training_data.shape[2]), return_sequences=False))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "    # training the LSTM model with the test dataset (test batches and dependent variables of the training datase)\n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "    model.fit(training_data, training_y_arr[l][i], epochs=1000, batch_size=64,\n",
        "                        validation_split=0.9, verbose=2, shuffle=False, callbacks=[earlyStopping])\n",
        "    \n",
        "    # making predictions by the LSTM model\n",
        "    yhat = model.predict(test_X_arr[l][i])\n",
        "\n",
        "    # recording predictions\n",
        "    LSTM_predictions.append(yhat.flatten('C'))\n",
        "    print('{} - done\\n\\n'.format(token_symbols[i]))\n",
        "\n",
        "  # recording all predictions\n",
        "  LSTM_predictions_for_all_lags.append(LSTM_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1COWhuWbCf4"
      },
      "source": [
        "#Calculate performance of ARIMA and LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3YdfYRPbGko"
      },
      "source": [
        "def mean_absolute_percentage_error(test_data, predicted_data): \n",
        "    return numpy.mean(numpy.abs((test_data - predicted_data) / test_data)) * 100\n",
        "\n",
        "def calculate_performance(predictions, test_data, scaler):\n",
        "  # rescaling the predictions to normal scale\n",
        "  dataframe_for_rescaling = DataFrame({\"col1\":predictions, \"col2\":predictions, \"col3\":predictions, \n",
        "                                       \"col4\":predictions, \"col5\":predictions, \"col6\":predictions})\n",
        "  dataframe_scaled_back = scaler.inverse_transform(dataframe_for_rescaling)\n",
        "  predictions_scaled_back = DataFrame(dataframe_scaled_back)[3].to_numpy()\n",
        "\n",
        "  # rescaling test dataset back to normal scale\n",
        "  dataframe_for_rescaling = DataFrame({\"col1\":test_data, \"col2\":test_data, \"col3\":test_data, \n",
        "                                       \"col4\":test_data, \"col5\":test_data, \"col6\":test_data})\n",
        "  dataframe_scaled_back = scaler.inverse_transform(dataframe_for_rescaling)\n",
        "  test_y_scaled_back = DataFrame(dataframe_scaled_back)[3].to_numpy()\n",
        "\n",
        "  # calculating performance metrics\n",
        "  RMSE = mean_squared_error(test_y_scaled_back, predictions_scaled_back, squared=False) # Root mean square error\n",
        "  MAE =  mean_absolute_error(test_y_scaled_back, predictions_scaled_back) # Mean absolute error\n",
        "  MAPE = mean_absolute_percentage_error(test_y_scaled_back, predictions_scaled_back) # Mean absolute percentage error\n",
        "  return [RMSE, MAE, MAPE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo1z_sc3bKaO"
      },
      "source": [
        "# array holding all results\n",
        "all_results = []\n",
        "\n",
        "# calculating and saving performance results for each lag option\n",
        "for l in range(len(lags_arr)):\n",
        "  lags = lags_arr[l]\n",
        "  print('{} lags:'.format(lags))\n",
        "\n",
        "  results = DataFrame(columns=['Token', 'ARIMA_RMSE','LSTM_RMSE', 'ARIMA_MAE', 'LSTM_MAE', 'ARIMA_MAPE', 'LSTM_MAPE'])\n",
        "\n",
        "  # calculating and saving performance results for each token\n",
        "  for i in range(number_of_tokens):\n",
        "    # calculating performance for a token\n",
        "    test_data = test_datasets[i]['Close'].to_numpy()\n",
        "    ARIMA_performance = calculate_performance(ARIMA_predictions_for_all_lags[l][i], test_data, scalers[i])\n",
        "    LSTM_performance = calculate_performance(LSTM_predictions_for_all_lags[l][i], test_data, scalers[i])\n",
        "\n",
        "    # recording results for a token\n",
        "    results = results.append({'Token': token_symbols[i], \n",
        "                              'ARIMA_RMSE': ARIMA_performance[0], 'LSTM_RMSE': LSTM_performance[0],\n",
        "                              'ARIMA_MAE': ARIMA_performance[1], 'LSTM_MAE': LSTM_performance[1], \n",
        "                              'ARIMA_MAPE': ARIMA_performance[2], 'LSTM_MAPE': LSTM_performance[2]}, \n",
        "                            ignore_index=True)\n",
        "  \n",
        "  # recording all results\n",
        "  all_results.append(results)\n",
        "  print(results)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0rUZb93jNgl9",
        "outputId": "3f128fb7-5def-451d-b91f-e39199b8e099"
      },
      "source": [
        "# downloading results as a file (optional)\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/drive')\n",
        "\n",
        "for i in range(len(lags_arr)):\n",
        "  fileName = 'results_ARIMA_LSTM_{}_lags.csv'.format(lags_arr[i])\n",
        "  all_results[i].to_csv(fileName)\n",
        "  files.download(fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_898e7330-74e2-403f-bded-c6788f52d39e\", \"results_ARIMA_LSTM_24_lags.csv\", 326)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6fc67630-3c23-4bc1-a395-79a74dbd98e0\", \"results_ARIMA_LSTM_48_lags.csv\", 325)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f837bdb2-7fb6-43af-8f95-727a0ec3aada\", \"results_ARIMA_LSTM_72_lags.csv\", 328)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KGZY8H5oMx6"
      },
      "source": [
        "# a method for plotting results\n",
        "def plotting_results(token_symbols, ARIMA_series, LSTM_series, metrics_name, number_of_lags):\n",
        "  # locations of the labels\n",
        "  x = numpy.arange(len(token_symbols))\n",
        "\n",
        "  # width of a bars\n",
        "  width = 0.35\n",
        "\n",
        "  # passing data for the chart\n",
        "  fig, ax = pyplot.subplots()\n",
        "  series1 = ax.bar(x - width/2, ARIMA_series, width, label='ARIMA')\n",
        "  series2 = ax.bar(x + width/2, LSTM_series, width, label='LSTM')\n",
        "\n",
        "  # adding labels, title, and formatting\n",
        "  ax.set_ylabel('Scaled logarithms of {}'.format(metrics_name))\n",
        "  ax.set_title('{} results for {} lags'.format(metrics_name, number_of_lags))\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(token_symbols)\n",
        "  ax.legend()\n",
        "  pyplot.rcParams[\"figure.figsize\"] = (12,3)\n",
        "  pyplot.xticks(rotation = 90)\n",
        "  pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZqtYT5_cTHA"
      },
      "source": [
        "# plotting charts with results for each lag option\n",
        "for i in range(len(all_results)):\n",
        "  # selecting list of tokens from the results\n",
        "  token_symbols = all_results[i]['Token']\n",
        "  number_of_tokens = len(token_symbols)\n",
        "\n",
        "  # concatenating ARIMA and LSTM results for scaling for better data visualisation\n",
        "  arima = all_results[i][['ARIMA_RMSE', 'ARIMA_MAE', 'ARIMA_MAPE']].to_numpy()\n",
        "  lstm = all_results[i][['LSTM_RMSE', 'LSTM_MAE', 'LSTM_MAPE']].to_numpy()\n",
        "  concatendated_results = DataFrame(numpy.concatenate((arima, lstm)), columns=['RMSE', 'MAE', 'MAPE'])\n",
        "\n",
        "  # computing logarithm of the results, so the scale is less diverge\n",
        "  log_results = numpy.log(concatendated_results)\n",
        "\n",
        "  # scaling the results data so it can be visualised on the same chart\n",
        "  scaler = MinMaxScaler()\n",
        "  scaledResults = DataFrame(scaler.fit_transform(log_results), columns=['RMSE', 'MAE', 'MAPE'])\n",
        "\n",
        "  # separating ARIMA and LSTM scaled results\n",
        "  ARIMA_scaled_results = scaledResults[:number_of_tokens]\n",
        "  LSTM_scaled_results = scaledResults[number_of_tokens:]\n",
        "\n",
        "  # Plotting charts\n",
        "  print('Results for {} lags:'.format(lags_arr[i]))\n",
        "  plotting_results(token_symbols, ARIMA_scaled_results['RMSE'], LSTM_scaled_results['RMSE'], 'RMSE', lags_arr[i])\n",
        "  plotting_results(token_symbols, ARIMA_scaled_results['MAE'], LSTM_scaled_results['MAE'], 'MAE', lags_arr[i])\n",
        "  plotting_results(token_symbols, ARIMA_scaled_results['MAPE'], LSTM_scaled_results['MAPE'], 'MAPE', lags_arr[i])\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}