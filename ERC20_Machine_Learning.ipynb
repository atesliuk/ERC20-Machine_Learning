{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERC20_Machine_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvz5pcyqs47p",
        "outputId": "4c28a8f2-08d1-4c91-eda4-cfc4e8b783a5"
      },
      "source": [
        "# importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import lag_plot\n",
        "from pandas import datetime, read_csv\n",
        "! pip install scikit-learn\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import sys\n",
        "\n",
        "# for ARIMA\n",
        "! pip install pmdarima\n",
        "import pmdarima as pm\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# for LSTM\n",
        "from math import sqrt\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "import seaborn as sns\n",
        "\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame\n",
        "from pandas import concat"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.7/dist-packages (1.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.24.3)\n",
            "Requirement already satisfied: numpy~=1.19.0 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.1.5)\n",
            "Requirement already satisfied: Cython!=0.29.18,>=0.29 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.29.24)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.4.1)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.22.2.post1)\n",
            "Requirement already satisfied: statsmodels!=0.12.0,>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->pmdarima) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19->pmdarima) (1.15.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels!=0.12.0,>=0.11->pmdarima) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMys94URZjhq"
      },
      "source": [
        "# importing data from csv files\n",
        "lags_arr = [24, 48, 72]\n",
        "token_symbols = ['UNI', 'LINK', 'AAVE', 'MKR', 'LEO', 'COMP', 'GRT', 'HT', 'CEL', \n",
        "                 'CHZ', 'TEL', 'YFI', 'HOT', 'ENJ', 'MANA', 'QNT', 'BAT', 'SNX', 'NEXO', \n",
        "                 'BNT', 'CRV', 'CHSB', 'KCS', 'ZRX', 'UMA', 'ANKR', 'VGX', '1INCH']\n",
        "number_of_tokens = len(token_symbols)\n",
        "raw_datasets = []\n",
        "\n",
        "for symbol in token_symbols:\n",
        "  raw_datasets.append(read_csv(symbol + '.csv'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c_LUicQ24Bq"
      },
      "source": [
        "# Scaling data and splitting to training and test datasets\n",
        "training_set_ratio = 0.80\n",
        "data_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Trades']\n",
        "scalers = []\n",
        "training_datasets, test_datasets = [], []\n",
        "training_datasets_dates, test_datasets_dates = [], []\n",
        "\n",
        "for i in range(number_of_tokens):\n",
        "  training_set_size = int(len(raw_datasets[i])*training_set_ratio)\n",
        "\n",
        "  # separating dates from values\n",
        "  training_datasets_dates.append(raw_datasets[i][:training_set_size]['Time'])\n",
        "  test_datasets_dates.append(raw_datasets[i][training_set_size:]['Time'])\n",
        "  raw_dataset_without_dates = raw_datasets[i][data_columns].astype('float')\n",
        "\n",
        "  # scaling\n",
        "  scaler = MinMaxScaler()\n",
        "  raw_training_dataset = raw_dataset_without_dates[:training_set_size]\n",
        "  scaler.fit(raw_training_dataset)\n",
        "  scaled_dataset = pd.DataFrame(scaler.transform(raw_dataset_without_dates), columns=data_columns)\n",
        "  scalers.append(scaler)\n",
        "\n",
        "  # splitting to training and test datasets\n",
        "  training_datasets.append(scaled_dataset[:training_set_size])\n",
        "  test_datasets.append(scaled_dataset[training_set_size:])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDdK5xetUGs9"
      },
      "source": [
        "#ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2rJXn-HO1ln",
        "outputId": "f8e0a10d-094d-4b5b-cc88-9c3b33bf3e8d"
      },
      "source": [
        "# Auto ARIMA to select the best parameters for the ARIMA model\n",
        "# documentation - http://alkaline-ml.com/pmdarima/1.8.1/modules/generated/pmdarima.arima.auto_arima.html#pmdarima.arima.auto_arima\n",
        "ARIMA_params = []\n",
        "\n",
        "for i in range(number_of_tokens):\n",
        "  model = pm.auto_arima(training_datasets[i]['Close'], \n",
        "                        test='adf', # use adftest to find optimal 'd'\n",
        "                        d=None, # let the model determine 'd'\n",
        "                        start_p=1, start_q=1,\n",
        "                        max_p=3, max_q=3, # maximum p=3 and q=3\n",
        "                        # trace=True, # to see logs in the process\n",
        "                        error_action='ignore',  \n",
        "                        suppress_warnings=True, \n",
        "                        stepwise=True)\n",
        "  print(token_symbols[i] + ' - ' + str(model.order))\n",
        "  ARIMA_params.append(model.order)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ENJ - (3, 1, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtGMiYPoZztk"
      },
      "source": [
        "# To ignore unrelated warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# predicting future prices with ARIMA\n",
        "ARIMA_predictions_for_all_lags = []\n",
        "\n",
        "for l in range(len(lags_arr)):\n",
        "  lags = lags_arr[l]\n",
        "  ARIMA_predictions = []\n",
        "  print('{} lags:'.format(lags))\n",
        "\n",
        "  for i in range(number_of_tokens):\n",
        "    test_dataset_close_prices = test_datasets[i]['Close'].to_numpy().tolist()\n",
        "    training_dataset_close_prices = training_datasets[i]['Close'].to_numpy().tolist()\n",
        "\n",
        "    training_X = []\n",
        "    for j in range(len(test_dataset_close_prices)):\n",
        "      curr_data = []\n",
        "      if j >= lags:\n",
        "        curr_data.extend(test_dataset_close_prices[(j-lags):j])\n",
        "      else:\n",
        "        curr_data.extend(training_dataset_close_prices[-(lags-j):])\n",
        "        curr_data.extend(test_dataset_close_prices[0:j])\n",
        "      training_X.append(curr_data)\n",
        "\n",
        "    curr_predictions = []\n",
        "    for j in range(len(test_dataset_close_prices)):\n",
        "      sys.stdout.write('\\r' + '{} - {}/{}'.format(token_symbols[i], j, len(test_dataset_close_prices)))\n",
        "      sys.stdout.flush()\n",
        "      model = ARIMA(training_X[j], order=ARIMA_params[i])\n",
        "      model_fit = model.fit()\n",
        "      output = model_fit.forecast()\n",
        "      yhat = output[0]\n",
        "      curr_predictions.append(yhat)\n",
        "      \n",
        "    ARIMA_predictions.append(curr_predictions)\n",
        "    print(' - done')\n",
        "\n",
        "  ARIMA_predictions_for_all_lags.append(ARIMA_predictions)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ls7Ptc5r1UB"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL_uDiTDDw-q"
      },
      "source": [
        "# prepare data\n",
        "training_y_arr, training_X_arr, test_X_arr = [], [], []\n",
        "\n",
        "for l in range(len(lags_arr)):\n",
        "  lags = lags_arr[l]\n",
        "\n",
        "  # test_y = []\n",
        "  training_y = []\n",
        "  training_X, test_X = [], []\n",
        "\n",
        "  for i in range(number_of_tokens):\n",
        "    training_y.append(training_datasets[i][lags:]['Close'].to_numpy())\n",
        "    # test_y.append(test_datasets[i][:]['Close'].to_numpy())\n",
        "\n",
        "    curr_training_X = []\n",
        "    for j in range(0, len(training_datasets[i]) - lags):\n",
        "      curr_training_X.append(training_datasets[i][j:j+lags].to_numpy().tolist())\n",
        "    training_X.append(np.array(curr_training_X))\n",
        "\n",
        "    curr_test_X = []\n",
        "    for j in range(len(test_datasets[i])):\n",
        "      curr_data = []\n",
        "      if j >= lags:\n",
        "        curr_data.extend(test_datasets[i][(j-lags):j].to_numpy().tolist())\n",
        "      else:\n",
        "        curr_data.extend(training_datasets[i][-(lags-j):].to_numpy().tolist())\n",
        "        curr_data.extend(test_datasets[i][0:j].to_numpy().tolist())\n",
        "      curr_test_X.append(curr_data)\n",
        "    test_X.append(np.array(curr_test_X))\n",
        "\n",
        "  training_y_arr.append(training_y)\n",
        "  training_X_arr.append(training_X)\n",
        "  test_X_arr.append(test_X)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHPbxn0fqxp"
      },
      "source": [
        "# build models and predict test values\n",
        "LSTM_predictions_for_all_lags = []\n",
        "\n",
        "for l in range(len(lags_arr)):\n",
        "  LSTM_predictions = []\n",
        "  print('{} lags:'.format(lags_arr[l]))\n",
        "\n",
        "  for i in range(number_of_tokens):\n",
        "    # design network\n",
        "\n",
        "    # determining number of neurons by a rule of thumb\n",
        "    alpha = 2\n",
        "    shape = training_X_arr[l][i].shape\n",
        "    neurons = int(shape[0] / (alpha * (shape[1] * shape[2] + 1)))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(neurons, input_shape=(training_X_arr[l][i].shape[1], training_X_arr[l][i].shape[2])))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "    # fit network\n",
        "    history = model.fit(training_X_arr[l][i], training_y_arr[l][i], epochs=400, batch_size=len(training_X_arr[l][i]), \n",
        "                        validation_split=0.9, verbose=0, shuffle=False)\n",
        "    \n",
        "    # make a prediction\n",
        "    yhat = model.predict(test_X_arr[l][i])\n",
        "    LSTM_predictions.append(yhat.flatten('C'))\n",
        "    print('{} - done, neurons - {}'.format(token_symbols[i], neurons))\n",
        "\n",
        "  LSTM_predictions_for_all_lags.append(LSTM_predictions)\n",
        "  print()\n",
        "\n",
        "    # plot history\n",
        "    # pyplot.plot(history.history['loss'], label='train')\n",
        "    # pyplot.plot(history.history['val_loss'], label='test')\n",
        "    # pyplot.legend()\n",
        "    # pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1COWhuWbCf4"
      },
      "source": [
        "#Calculate performance of ARIMA and LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3YdfYRPbGko"
      },
      "source": [
        "def calculate_MAPE(test_data, predicted_data): \n",
        "    return np.mean(np.abs((test_data - predicted_data) / test_data)) * 100\n",
        "\n",
        "def calculate_performance(predictions, test_data, scaler):\n",
        "  # rescaling back predictions\n",
        "  dataframe_for_rescaling = DataFrame({\"col1\":predictions, \"col2\":predictions, \"col3\":predictions, \n",
        "                                       \"col4\":predictions, \"col5\":predictions, \"col6\":predictions})\n",
        "  dataframe_scaled_back = scaler.inverse_transform(dataframe_for_rescaling)\n",
        "  predictions_scaled_back = DataFrame(dataframe_scaled_back)[3].to_numpy()\n",
        "\n",
        "  # Rescaling back test dataset\n",
        "  dataframe_for_rescaling = DataFrame({\"col1\":test_data, \"col2\":test_data, \"col3\":test_data, \n",
        "                                       \"col4\":test_data, \"col5\":test_data, \"col6\":test_data})\n",
        "  dataframe_scaled_back = scaler.inverse_transform(dataframe_for_rescaling)\n",
        "  test_y_scaled_back = DataFrame(dataframe_scaled_back)[3].to_numpy()\n",
        "\n",
        "  # calculating performance metrics\n",
        "  RMSE = mean_squared_error(test_y_scaled_back, predictions_scaled_back, squared=False) # Root mean square error\n",
        "  MAE =  mean_absolute_error(test_y_scaled_back, predictions_scaled_back) # Mean absolute error\n",
        "  MAPE = calculate_MAPE(test_y_scaled_back, predictions_scaled_back) # Mean absolute percentage error\n",
        "  return [RMSE, MAE, MAPE]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo1z_sc3bKaO"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/drive')\n",
        "\n",
        "# Calculating and saving performance results\n",
        "for l in range(len(lags_arr)):\n",
        "  lags = lags_arr[l]\n",
        "  print('{} lags:'.format(lags))\n",
        "\n",
        "  results = DataFrame(columns=['Token', 'ARIMA_RMSE','LSTM_RMSE', 'ARIMA_MAE', 'LSTM_MAE', 'ARIMA_MAPE', 'LSTM_MAPE'])\n",
        "\n",
        "  for i in range(number_of_tokens):\n",
        "    # calculating performance\n",
        "    test_data = test_datasets[i]['Close'].to_numpy()\n",
        "    ARIMA_performance = calculate_performance(ARIMA_predictions_for_all_lags[l][i], test_data, scalers[i])\n",
        "    LSTM_performance = calculate_performance(LSTM_predictions_for_all_lags[l][i], test_data, scalers[i])\n",
        "\n",
        "    results = results.append({'Token': token_symbols[i], \n",
        "                              'ARIMA_RMSE': ARIMA_performance[0], 'LSTM_RMSE': LSTM_performance[0],\n",
        "                              'ARIMA_MAE': ARIMA_performance[1], 'LSTM_MAE': LSTM_performance[1], \n",
        "                              'ARIMA_MAPE': ARIMA_performance[2], 'LSTM_MAPE': LSTM_performance[2]}, \n",
        "                            ignore_index=True)\n",
        "  print(results)\n",
        "  print()\n",
        "  fileName = 'results_ARIMA_LSTM_{}_lags.csv'.format(lags)\n",
        "  results.to_csv(fileName)\n",
        "  files.download(fileName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZqtYT5_cTHA"
      },
      "source": [
        "# test_set_range = df[int(len(df)*training_set_ratio):].index\n",
        "# plt.plot(test_set_range, model_predictions, color='blue', marker='o', linestyle='dashed',label='Predicted Price')\n",
        "# plt.plot(test_set_range, test_data, color='red', label='Actual Price')\n",
        "# plt.title('UNI Prices Prediction')\n",
        "# plt.xlabel('Date')\n",
        "# plt.ylabel('Prices')\n",
        "# plt.xticks(np.arange(3911,5587,200), df.Date[3911:5587:200])\n",
        "# plt.xticks(rotation = 90)\n",
        "# plt.rcParams[\"figure.figsize\"] = (10,3)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}